{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab5_orig.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FRVhSP4igMy",
        "colab_type": "text"
      },
      "source": [
        "Part 1:\n",
        "\n",
        "Done:\n",
        "-Use pretrained VGG\n",
        "-Gather statistics from content image\n",
        "-Gather statistics from style image\n",
        "\n",
        "Part 2:\n",
        "\n",
        "Done:\n",
        "-Display style tensor and content tensor transformed back into image\n",
        "\n",
        "Part 3:\n",
        "\n",
        "Done:\n",
        "-Create classes for style and content losses\n",
        "\n",
        "Part 4:\n",
        "\n",
        "Done:\n",
        "-Used a BFGS optimizer\n",
        "-Showed the content and style loss every 50 steps\n",
        "-Clamp the outputs\n",
        "-display the tensor as an image\n",
        "![alt text](https://nolans-cs-bucket.s3-us-west-1.amazonaws.com/ezgif.com-gif-maker.gif)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQd8r6_X1ct2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, utils, datasets\n",
        "from tqdm import tqdm\n",
        "from torch.nn.parameter import Parameter\n",
        "import pdb\n",
        "import torchvision\n",
        "import os\n",
        "import gzip\n",
        "import tarfile\n",
        "import gc\n",
        "from PIL import Image\n",
        "import io\n",
        "from IPython.core.ultratb import AutoFormattedTB\n",
        "__ITB__ = AutoFormattedTB(mode = 'Verbose', color_scheme='LightBg', tb_offset = 1)\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import pdb\n",
        "from torch.utils import data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYvkhgBghzFH",
        "colab_type": "text"
      },
      "source": [
        "Part 1:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK9D2Ft91g4V",
        "colab_type": "code",
        "outputId": "4b14831f-931b-4fda-d94c-8413e9279fcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        }
      },
      "source": [
        "load_and_normalize = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((250,250)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "def upload():\n",
        "  print('Upload Content Image')\n",
        "  file_dict = files.upload()\n",
        "  content_path = io.BytesIO(file_dict[next(iter(file_dict))])\n",
        "\n",
        "  print('\\nUpload Style Image')\n",
        "  file_dict = files.upload()\n",
        "  style_path = io.BytesIO(file_dict[next(iter(file_dict))])\n",
        "  return content_path, style_path\n",
        "\n",
        "content_path, style_path = upload()\n",
        "\n",
        "print(\"Content Path: {}\".format(content_path))\n",
        "print(\"Style Path: {}\".format(style_path))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Upload Content Image\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-62bddb50-3f94-49e3-bec3-f90dea74fbba\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-62bddb50-3f94-49e3-bec3-f90dea74fbba\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7b1177a46750>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcontent_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mcontent_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content Path: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-7b1177a46750>\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Upload Content Image'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mcontent_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m   result = _output.eval_js(\n\u001b[1;32m     63\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[0;32m---> 64\u001b[0;31m           input_id=input_id, output_id=output_id))\n\u001b[0m\u001b[1;32m     65\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: google.colab._files is undefined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mjy1P3aQ1olF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content_image_orig = Image.open(content_path)\n",
        "content_image = load_and_normalize(np.array(content_image_orig)).unsqueeze(0).cuda()\n",
        "style_image_orig = Image.open(style_path)\n",
        "style_image = load_and_normalize(np.array(style_image_orig)).unsqueeze(0).cuda()\n",
        "dim=content_image.size(3)\n",
        "import random\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXiQGHtl1zLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vgg_names = [\"conv1_1\", \"relu1_1\", \"conv1_2\", \"relu1_2\", \"maxpool1\", \"conv2_1\", \"relu2_1\", \"conv2_2\", \"relu2_2\", \"maxpool2\", \"conv3_1\", \"relu3_1\", \"conv3_2\", \"relu3_2\", \"conv3_3\", \"relu3_3\",\"maxpool3\", \"conv4_1\", \"relu4_1\", \"conv4_2\", \"relu4_2\", \"conv4_3\", \"relu4_3\",\"maxpool4\", \"conv5_1\", \"relu5_1\", \"conv5_2\", \"relu5_2\", \"conv5_3\", \"relu5_3\",\"maxpool5\"]\n",
        "\n",
        "vgg_names_style = [\"conv2_1\",\"conv3_1\", \"conv4_1\", \n",
        "                  \"conv5_1\"]\n",
        "\n",
        "style_idx=[]\n",
        "for things in vgg_names_style:\n",
        "  style_idx.append(vgg_names.index(things))\n",
        "\n",
        "vgg_names_content = [\"conv4_1\"]\n",
        "\n",
        "cont_idx=[]\n",
        "for things in vgg_names_content:\n",
        "  cont_idx.append(vgg_names.index(things))\n",
        "\n",
        "# Choose the layers to use for style and content transfer\n",
        "\n",
        "# Create the vgg network in eval mode\n",
        "#  with our forward method that returns the outputs of the intermediate layers we requested\n",
        "\n",
        "\n",
        "\n",
        "# Cache the outputs of the content and style layers for their respective images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4FQb85r1ucO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "class Normalization(nn.Module):\n",
        "  def __init__(self, mean=torch.tensor([0.485, 0.456, 0.406]).cuda(), std=torch.tensor([0.229, 0.224, 0.225]).cuda()):\n",
        "      super(Normalization, self).__init__()\n",
        "      self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
        "      self.std = torch.tensor(std).view(-1, 1, 1)\n",
        "\n",
        "  def forward(self, img):\n",
        "      return (img - self.mean) / self.std\n",
        "\n",
        "class VGGIntermediate(nn.Module):\n",
        "  def __init__(self, requested=[]):\n",
        "    super(VGGIntermediate, self).__init__()\n",
        "    self.norm = Normalization().eval()\n",
        "    self.intermediates = {}\n",
        "    self.vgg = models.vgg16(pretrained=True).features.eval()\n",
        "    for i, m in enumerate(self.vgg.children()):\n",
        "        if isinstance(m, nn.ReLU):   # we want to set the relu layers to NOT do the relu in place. \n",
        "          m.inplace = False          # the model has a hard time going backwards on the in place functions. \n",
        "        \n",
        "        if isinstance(m,nn.MaxPool2d):\n",
        "          #pdb.set_trace()\n",
        "          m=nn.AvgPool2d(2,stride=2)\n",
        "        \n",
        "        if i in requested:\n",
        "          def curry(i):\n",
        "            def hook(module, input, output):\n",
        "              self.intermediates[i] = output\n",
        "            return hook\n",
        "          m.register_forward_hook(curry(i))\n",
        "    \n",
        "  def forward(self, x):\n",
        "    self.vgg(self.norm(x))  \n",
        "    return self.intermediates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSqqN5TzjsIl",
        "colab_type": "text"
      },
      "source": [
        "Part 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWPT4uIV11iD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "toPIL = transforms.ToPILImage()  \n",
        "\n",
        "def display(tensor, title=None):\n",
        "    image = tensor.cpu().clone()  \n",
        "    image = image.squeeze(0)    \n",
        "    image = toPIL(image)\n",
        "    plt.imshow(image)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "\n",
        "plt.figure()\n",
        "display(style_image, title='Style Image')\n",
        "\n",
        "plt.figure()\n",
        "display(content_image, title='Content Image')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ27A5bIj5aI",
        "colab_type": "text"
      },
      "source": [
        "Part 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG8VcRpj13wu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gram_matrix(input):\n",
        "  #pdb.set_trace()\n",
        "  a,b,c,d = input.size()\n",
        "  #a is the batch size\n",
        "  #b is number f feature maps\n",
        "  #c/d are dimensions of f map\n",
        "  var = input.view(a*b,c*d)\n",
        "  \n",
        "  #test=np.random.shuffle(var)\n",
        "  idx = torch.randperm(var.nelement())\n",
        "  var = var.view(-1)[idx].view(var.size())\n",
        "  var2 = torch.mm(var,var.t())\n",
        "  \n",
        "  out = var2.div(a*b*c*d)\n",
        "    \n",
        "  \n",
        "  return out\n",
        "   \n",
        "  \n",
        "class ContentLoss(nn.Module):\n",
        "  def __init__(self, target):\n",
        "    super(ContentLoss,self).__init__()\n",
        "    self.target=target.detach()\n",
        "       \n",
        "  def forward(self,input,target_c):\n",
        "    \n",
        "    loss=torch.nn.functional.mse_loss(input[1],target_c[1])\n",
        "    #pdb.set_trace()\n",
        "    return loss#shouldn't this be loss?\n",
        "    \n",
        "class StyleLoss(nn.Module):\n",
        "  def __init__(self,target_s):\n",
        "    super(StyleLoss,self).__init__()\n",
        "    \n",
        "    self.target=gram_matrix(target_s).detach()\n",
        "    \n",
        "  def forward(self,input,target):\n",
        "    G_input=gram_matrix(input[1])\n",
        "    G_target=gram_matrix(target[1])\n",
        "   \n",
        "    loss=torch.nn.functional.mse_loss(G_input,G_target)\n",
        "    return loss\n",
        "\n",
        "content_layers=VGGIntermediate(cont_idx).cuda()\n",
        "\n",
        "style_layers=VGGIntermediate(style_idx).cuda()\n",
        "\n",
        "styleloss=StyleLoss(style_image)\n",
        "contentloss=ContentLoss(content_image)\n",
        "wl=0.20\n",
        "#create white noise image with the same size\n",
        "\n",
        "\n",
        "def get_losses(content_image,style_image,x,cont_idx,style_idx,content_layers,style_layers,wl):\n",
        "\n",
        "  #get the single content layer\n",
        "  P_out=list(content_layers(content_image).items())\n",
        "  PX_out=list(content_layers(x).items())\n",
        "  #get the multiple style layers\n",
        "  S_out=list(style_layers(style_image).items())\n",
        "  X_out=list(style_layers(x).items())\n",
        "\n",
        "  loss_v=[]\n",
        "  loss_c=[]\n",
        "  #calculate the gram matrix for the style layers\n",
        "  for i in range(len(S_out)):\n",
        "   \n",
        "    loss=styleloss(X_out[i],S_out[i])\n",
        "    loss_v.append(loss*wl)\n",
        "    #pdb.set_trace()\n",
        "  loss_v_sum=sum(loss_v)\n",
        "  \n",
        "  for i in range(len(P_out)):\n",
        "    #pdb.set_trace()\n",
        "    loss=contentloss(PX_out[i],P_out[i])\n",
        "    \n",
        "    loss_c.append(loss)\n",
        "  loss_c_sum=sum(loss_c)\n",
        "    \n",
        "  #calculate the loss for the style image\n",
        "  \n",
        "  \n",
        "  #return the losses of each image\n",
        "  return(loss_v_sum,loss_c_sum)\n",
        "\n",
        "#white_image1=load_and_normalize(np.array([[random.random()*254 for i in range(dim)] for j in range(dim)],dtype='float32')).cuda()\n",
        "#white_image2=load_and_normalize(np.array([[random.random()*254 for i in range(dim)] for j in range(dim)],dtype='float32')).cuda()\n",
        "#white_image3=load_and_normalize(np.array([[random.random()*254 for i in range(dim)] for j in range(dim)],dtype='float32')).cuda()\n",
        "content_copy=content_image\n",
        "\n",
        "\n",
        "\n",
        "#test=(white_image.cpu().detach().numpy())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate a content loss module for each content layer \n",
        "#  with the content reference image outputs for that layer for comparison\n",
        "\n",
        "# Instantiate a sytle loss module for each style layer \n",
        "#  with the style reference image outputs for that layer for comparison"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ngm3u6Yc8WTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VGGIntermediate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYg2ldllj8Ze",
        "colab_type": "text"
      },
      "source": [
        "Part 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG9uvX2Sh1yT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs=100\n",
        "#white_image1=load_and_normalize(np.array([[random.random()*254 for i in range(dim)] for j in range(dim)],dtype='float32')).cuda()\n",
        "#white_image2=load_and_normalize(np.array([[random.random()*254 for i in range(dim)] for j in range(dim)],dtype='float32')).cuda()\n",
        "#white_image3=load_and_normalize(np.array([[random.random()*254 for i in range(dim)] for j in range(dim)],dtype='float32')).cuda()\n",
        "\n",
        "#white_image=torch.cat([white_image1,white_image2,white_image3]).unsqueeze(0)\n",
        "#style_weight=1000\n",
        "#content_weight=1000\n",
        "content_copy=content_image\n",
        "\n",
        "from google.colab import drive\n",
        "def save_images(white_image,i):\n",
        "  \n",
        "  test=content_copy.squeeze(0)\n",
        "  test2=test.cpu()\n",
        "  test3=test2.detach().numpy()\n",
        "  test3=test3\n",
        "  r=test3[0]\n",
        "  g=test3[1]\n",
        "  b=test3[2]\n",
        "\n",
        "  image=np.zeros((dim,dim,3))\n",
        "  image[:,:,0]=r\n",
        "  image[:,:,1]=g\n",
        "  image[:,:,2]=b\n",
        "  \n",
        "  plt.imshow(image)\n",
        "\n",
        "  drive.mount('/content/drive')\n",
        "  with open('/content/drive/My Drive/stylized_image_' + str(i) + '.png', 'w') as f:\n",
        "    out=plt.savefig('/content/drive/My Drive/stylized_image_' + str(i) + '.png')\n",
        "\n",
        "\n",
        "def get_input_optimizer(input_img):\n",
        "    # this line to show that input is a parameter that requires a gradient\n",
        "    optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
        "    return optimizer\n",
        "  \n",
        "def style_transfer(content_image,style_image,content_copy,cont_idx,style_idx,content_layers,style_layers,wl,epochs,num_steps=50,\n",
        "                       style_weight=5000, content_weight=1):\n",
        "  optimizer=get_input_optimizer(content_copy)\n",
        "  #pdb.set_trace()\n",
        " \n",
        "  for i in range(epochs):\n",
        "    \n",
        "    def loop():\n",
        "      \n",
        "      content_copy.data.clamp_(0, 1)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      loss_s,loss_c=get_losses(content_image,style_image,content_copy,cont_idx,style_idx,content_layers,style_layers,wl)\n",
        "\n",
        "      loss_total=content_weight*loss_c+style_weight*loss_s\n",
        "      #pdb.set_trace()\n",
        "      loss_total.backward()\n",
        "      print(loss_total)\n",
        "      return(loss_total)\n",
        "    optimizer.step(loop)\n",
        "    content_copy.data.clamp_(0,1) \n",
        "    save_images(content_copy,i)\n",
        "    print(\"images saved: \" + str(i))\n",
        "  \n",
        "    \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "style_transfer(content_image,style_image,content_copy,cont_idx,style_idx,content_layers,style_layers,wl,epochs)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}